{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70c31e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pytorch-msssim\n",
    "# %pip install torchmetrics\n",
    "# %pip install lpips\n",
    "# %pip install h5py\n",
    "# %pip install pathos\n",
    "# %pip install multiprocess\n",
    "# %pip install lmdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15f1f814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jul  3 04:28:06 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 576.52                 Driver Version: 576.52         CUDA Version: 12.9     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1650 Ti   WDDM  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   55C    P5              5W /   50W |      89MiB /   4096MiB |     27%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A            3008    C+G   ...ms\\Microsoft VS Code\\Code.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acc3a1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Taneem\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import gc\n",
    "import cv2\n",
    "import lpips\n",
    "import lmdb\n",
    "import time\n",
    "import shutil\n",
    "import random\n",
    "import torchmetrics\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.multiprocessing as mp\n",
    "import torchvision.utils as vutils\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "from pytorch_msssim import MS_SSIM\n",
    "from multiprocess.pool import ThreadPool\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics.image import PeakSignalNoiseRatio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "372de570",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp.set_start_method('spawn', force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7e5571b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available(\n",
    ") else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.deterministic = False\n",
    "\n",
    "COLORBLIND_TYPES = [\"protanopia\", \"deuteranopia\", \"tritanopia\"]\n",
    "NUM_EPOCHS = 50\n",
    "BATCH_SIZE = 16\n",
    "IMG_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8b0021f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(file_batch, data_dir, cb_type, img_size):\n",
    "    original_dir = os.path.join(data_dir, \"original\")\n",
    "    type_dir = os.path.join(data_dir, cb_type)\n",
    "\n",
    "    orig_batch = []\n",
    "    cb_batch = []\n",
    "\n",
    "    for fname in file_batch:\n",
    "        try:\n",
    "            orig_img = cv2.imread(os.path.join(original_dir, fname))\n",
    "            if orig_img is None:\n",
    "                raise FileNotFoundError(f\"Original image {fname} not found\")\n",
    "            orig_img = cv2.cvtColor(orig_img, cv2.COLOR_BGR2RGB)\n",
    "            orig_img = cv2.resize(orig_img, (img_size, img_size))\n",
    "\n",
    "            cb_img = cv2.imread(os.path.join(type_dir, fname))\n",
    "            if cb_img is None:\n",
    "                raise FileNotFoundError(f\"Colorblind image {fname} not found\")\n",
    "            cb_img = cv2.cvtColor(cb_img, cv2.COLOR_BGR2RGB)\n",
    "            cb_img = cv2.resize(cb_img, (img_size, img_size))\n",
    "\n",
    "            orig_batch.append(orig_img)\n",
    "            cb_batch.append(cb_img)\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError processing {fname}: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    return np.array(orig_batch), np.array(cb_batch)\n",
    "\n",
    "\n",
    "def preprocess_type(data_dir, cb_type, output_path, num_workers=6, batch_size=1000):\n",
    "    \"\"\"Preprocess and write LMDB for one colorblind type\"\"\"\n",
    "    type_dir = os.path.join(data_dir, cb_type)\n",
    "    files = [f for f in os.listdir(type_dir)\n",
    "             if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "    if not files:\n",
    "        raise ValueError(f\"No images found in {type_dir}\")\n",
    "\n",
    "    batches = [files[i:i + batch_size]\n",
    "               for i in range(0, len(files), batch_size)]\n",
    "\n",
    "    env = None\n",
    "    try:\n",
    "        # Estimate needed map size\n",
    "        map_size = 15 * 1024 ** 3  # 15 GB max\n",
    "        env = lmdb.open(output_path, map_size=map_size)\n",
    "\n",
    "        with env.begin(write=True) as txn:\n",
    "            with ThreadPool(num_workers) as p:\n",
    "                with tqdm(total=len(batches), desc=f\"Processing {cb_type}\") as pbar:\n",
    "                    process_func = partial(process_batch,\n",
    "                                           data_dir=data_dir,\n",
    "                                           cb_type=cb_type,\n",
    "                                           img_size=IMG_SIZE)\n",
    "\n",
    "                    total_idx = 0\n",
    "                    for orig_data, cb_data in p.imap(process_func, batches):\n",
    "                        for img_idx in range(len(orig_data)):\n",
    "                            txn.put(f'original_{total_idx}'.encode(),\n",
    "                                    orig_data[img_idx].tobytes())\n",
    "                            txn.put(f'{cb_type}_{total_idx}'.encode(),\n",
    "                                    cb_data[img_idx].tobytes())\n",
    "                            total_idx += 1\n",
    "                        pbar.update(1)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during preprocessing {cb_type}: {str(e)}\")\n",
    "        if os.path.exists(output_path):\n",
    "            print(f\"Deleting incomplete LMDB {output_path}\")\n",
    "            # Add retry loop for Windows\n",
    "            for _ in range(3):\n",
    "                try:\n",
    "                    shutil.rmtree(output_path, ignore_errors=True)\n",
    "                    break\n",
    "                except PermissionError:\n",
    "                    time.sleep(0.5)\n",
    "        raise\n",
    "    finally:\n",
    "        if env is not None:\n",
    "            env.close()\n",
    "            # Add this for Windows compatibility\n",
    "            if os.name == 'nt':\n",
    "                try:\n",
    "                    os.remove(os.path.join(output_path, 'lock.mdb'))\n",
    "                except Exception as e:\n",
    "                    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "beab4c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorblindDataset(Dataset):\n",
    "    def __init__(self, lmdb_path, cb_type, transform=None):\n",
    "        self.lmdb_path = os.path.abspath(lmdb_path)\n",
    "        self.cb_type = cb_type\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "        ])\n",
    "        self._env = None  # Will be initialized per worker\n",
    "\n",
    "        # Use temporary environment to count entries\n",
    "        with lmdb.open(self.lmdb_path, readonly=True, lock=False) as env:\n",
    "            with env.begin() as txn:\n",
    "                self._len = txn.stat()['entries'] // 2\n",
    "                if self._len == 0:\n",
    "                    raise ValueError(f\"LMDB {lmdb_path} contains 0 samples.\")\n",
    "\n",
    "    def _init_env(self):\n",
    "        if self._env is None:\n",
    "            self._env = lmdb.open(\n",
    "                self.lmdb_path, readonly=True, lock=False, max_readers=128)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            self._init_env()\n",
    "            with self._env.begin(buffers=True) as txn:\n",
    "                orig_bytes = txn.get(f'original_{idx}'.encode())\n",
    "                cb_bytes = txn.get(f'{self.cb_type}_{idx}'.encode())\n",
    "\n",
    "                if orig_bytes is None or cb_bytes is None:\n",
    "                    raise RuntimeError(f\"Missing data for index {idx}\")\n",
    "\n",
    "                orig = np.frombuffer(orig_bytes, dtype=np.uint8).reshape(\n",
    "                    IMG_SIZE, IMG_SIZE, 3)\n",
    "                cb = np.frombuffer(cb_bytes, dtype=np.uint8).reshape(\n",
    "                    IMG_SIZE, IMG_SIZE, 3)\n",
    "\n",
    "            orig = torch.from_numpy(orig.copy()).permute(\n",
    "                2, 0, 1).float().div(255)\n",
    "            cb = torch.from_numpy(cb.copy()).permute(2, 0, 1).float().div(255)\n",
    "\n",
    "            if self.transform:\n",
    "                orig = self.transform(orig)\n",
    "                cb = self.transform(cb)\n",
    "\n",
    "            return orig, cb\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading index {idx}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._len\n",
    "\n",
    "    def close(self):\n",
    "        if self._env is not None:\n",
    "            self._env.close()\n",
    "            self._env = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b96c2902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoencoder Model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Encoder\n",
    "        self.enc1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32), nn.ReLU(True))\n",
    "        self.enc2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64), nn.ReLU(True))\n",
    "        self.enc3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128), nn.ReLU(True))\n",
    "        self.enc4 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256), nn.ReLU(True))\n",
    "        self.enc5 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512), nn.ReLU(True))\n",
    "\n",
    "        # Decoder with Skip Connections\n",
    "        self.dec5 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 256, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256), nn.ReLU(True))\n",
    "        self.dec4 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 128, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128), nn.ReLU(True))\n",
    "        self.dec3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 64, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64), nn.ReLU(True))\n",
    "        self.dec2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 32, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32), nn.ReLU(True))\n",
    "        self.dec1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 3, 4, stride=2, padding=1),\n",
    "            nn.Tanh())\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        e1 = self.enc1(x)  # 128\n",
    "        e2 = self.enc2(e1)  # 64\n",
    "        e3 = self.enc3(e2)  # 32\n",
    "        e4 = self.enc4(e3)  # 16\n",
    "        e5 = self.enc5(e4)  # 8\n",
    "\n",
    "        # Decoder\n",
    "        d5 = self.dec5(e5)\n",
    "        d4 = self.dec4(torch.cat([d5, e4], 1))\n",
    "        d3 = self.dec3(torch.cat([d4, e3], 1))\n",
    "        d2 = self.dec2(torch.cat([d3, e2], 1))\n",
    "        d1 = self.dec1(torch.cat([d2, e1], 1))\n",
    "        return d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbdfca73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Utilities\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, delta=0.001):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if (self.best_loss - val_loss) > self.delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "\n",
    "def process_image(tensor):\n",
    "    \"\"\"Universal image processing function for proper denormalization\"\"\"\n",
    "    img = tensor.detach().cpu().numpy()\n",
    "    img = img.transpose(1, 2, 0)  # CHW to HWC\n",
    "    img = np.clip(img, -1, 1)     # Ensure values are in [-1, 1]\n",
    "    img = (img * 0.5 + 0.5)       # Scale to [0, 1]\n",
    "    img = (img * 255).astype(np.uint8)\n",
    "    return img\n",
    "\n",
    "# Visualization with Saving\n",
    "\n",
    "\n",
    "def visualize_and_save(orig, target, output, cb_type, epoch, batch_idx, save_dir):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    for ax, img, title in zip(axes, [orig, target, output], ['Original', 'TargetDaltonized', 'Generated']):\n",
    "        processed_img = process_image(img)\n",
    "        ax.imshow(processed_img)\n",
    "        ax.set_title(f\"{title} ({cb_type})\")\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.savefig(os.path.join(save_dir, f\"{cb_type}_e{epoch+1}_b{batch_idx}.png\"),\n",
    "                bbox_inches='tight',\n",
    "                dpi=100)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51b8f8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker_init_fn(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3bd537ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified DataLoader setup\n",
    "def get_loader(dataset, batch_size, num_workers, shuffle):\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        worker_init_fn=worker_init_fn,\n",
    "        persistent_workers=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81508065",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_colorblind_type(cb_type):\n",
    "    train_lmdb_path = f\"temp_train_{cb_type}.lmdb\"\n",
    "    val_lmdb_path = f\"temp_val_{cb_type}.lmdb\"\n",
    "    metric_path = f\"metrics_{cb_type}.npz\"\n",
    "    checkpoint_path = f\"checkpoint_{cb_type}.pth\"\n",
    "\n",
    "    try:\n",
    "        # Check for existing checkpoint\n",
    "        resume = os.path.exists(checkpoint_path)\n",
    "\n",
    "        if not resume:\n",
    "            # Fresh training setup\n",
    "            print(f\"\\n{'='*40}\\nPreprocessing {cb_type}\\n{'='*40}\")\n",
    "            if not os.path.exists(train_lmdb_path):\n",
    "                preprocess_type(\"data/train\", cb_type, train_lmdb_path)\n",
    "            if not os.path.exists(val_lmdb_path):\n",
    "                preprocess_type(\"data/val\", cb_type, val_lmdb_path)\n",
    "\n",
    "            # Validate LMDBs only for fresh start\n",
    "            for path in [train_lmdb_path, val_lmdb_path]:\n",
    "                with lmdb.open(path, readonly=True) as env:\n",
    "                    with env.begin() as txn:\n",
    "                        entries = txn.stat()[\"entries\"]\n",
    "                        if entries == 0:\n",
    "                            raise RuntimeError(f\"LMDB {path} is empty!\")\n",
    "                        print(f\"{path} has {entries//2} samples\")\n",
    "\n",
    "        # Initialize model and components\n",
    "        model = Autoencoder().to(DEVICE)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "        scaler = GradScaler()\n",
    "        early_stopper = EarlyStopping(patience=5)\n",
    "        ms_ssim = MS_SSIM(data_range=2.0).to(DEVICE)\n",
    "        psnr = PeakSignalNoiseRatio().to(DEVICE)\n",
    "        lpips_model = lpips.LPIPS(net='alex').to(DEVICE)\n",
    "\n",
    "        start_epoch = 0\n",
    "        metrics = {\n",
    "            'train_loss': [], 'val_loss': [],\n",
    "            'train_psnr': [], 'val_psnr': [],\n",
    "            'train_lpips': [], 'val_lpips': []\n",
    "        }\n",
    "\n",
    "        # Resume logic\n",
    "        if resume:\n",
    "            print(f\"\\n{'='*40}\\nResuming {cb_type} training\\n{'='*40}\")\n",
    "            checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            scaler.load_state_dict(checkpoint['scaler_state_dict'])\n",
    "            early_stopper.counter = checkpoint['early_stop_counter']\n",
    "            start_epoch = checkpoint['epoch'] + 1\n",
    "            metrics = checkpoint['metrics']\n",
    "\n",
    "            print(f\"Resuming from epoch {start_epoch} with:\")\n",
    "            print(f\"- Best val loss: {min(metrics['val_loss']):.4f}\")\n",
    "            print(f\"- Current early stop counter: {early_stopper.counter}\")\n",
    "\n",
    "        # Data loading (always needed)\n",
    "        train_dataset = ColorblindDataset(train_lmdb_path, cb_type)\n",
    "        val_dataset = ColorblindDataset(val_lmdb_path, cb_type)\n",
    "        train_loader = get_loader(\n",
    "            train_dataset, BATCH_SIZE, num_workers=0, shuffle=True)\n",
    "        val_loader = get_loader(val_dataset, BATCH_SIZE,\n",
    "                                num_workers=0, shuffle=False)\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(start_epoch, NUM_EPOCHS):\n",
    "            model.train()\n",
    "            epoch_loss = epoch_psnr = epoch_lpips = 0.0\n",
    "\n",
    "            # Training phase\n",
    "            for batch_idx, (orig, target) in tqdm(enumerate(train_loader),\n",
    "                                                  desc=f\"{cb_type} Epoch {epoch+1}\",\n",
    "                                                  total=len(train_loader)):\n",
    "                orig, target = orig.to(DEVICE), target.to(DEVICE)\n",
    "\n",
    "                with autocast():\n",
    "                    output = model(orig)\n",
    "                    loss = 1 - ms_ssim(output, target)\n",
    "\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    output_scaled = (output + 1) / 2\n",
    "                    target_scaled = (target + 1) / 2\n",
    "                    psnr_val = psnr(output_scaled, target_scaled)\n",
    "                    lpips_val = lpips_model(output, target).mean()\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "                epoch_psnr += psnr_val.item()\n",
    "                epoch_lpips += lpips_val.item()\n",
    "\n",
    "                if batch_idx % 500 == 0:\n",
    "                    visualize_and_save(orig[0], target[0], output[0], cb_type,\n",
    "                                       epoch, batch_idx, \"training_samples\")\n",
    "\n",
    "            # Update metrics\n",
    "            metrics['train_loss'].append(epoch_loss / len(train_loader))\n",
    "            metrics['train_psnr'].append(epoch_psnr / len(train_loader))\n",
    "            metrics['train_lpips'].append(epoch_lpips / len(train_loader))\n",
    "\n",
    "            # Validation phase\n",
    "            model.eval()\n",
    "            val_loss = val_psnr = val_lpips = 0.0\n",
    "            with torch.no_grad():\n",
    "                for orig, target in val_loader:\n",
    "                    orig, target = orig.to(DEVICE), target.to(DEVICE)\n",
    "                    output = model(orig)\n",
    "\n",
    "                    loss = 1 - ms_ssim(output, target)\n",
    "                    output_scaled = (output + 1) / 2\n",
    "                    target_scaled = (target + 1) / 2\n",
    "\n",
    "                    val_loss += loss.item()\n",
    "                    val_psnr += psnr(output_scaled, target_scaled).item()\n",
    "                    val_lpips += lpips_model(output, target).mean().item()\n",
    "\n",
    "            metrics['val_loss'].append(val_loss / len(val_loader))\n",
    "            metrics['val_psnr'].append(val_psnr / len(val_loader))\n",
    "            metrics['val_lpips'].append(val_lpips / len(val_loader))\n",
    "\n",
    "            # Save checkpoint\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scaler_state_dict': scaler.state_dict(),\n",
    "                'early_stop_counter': early_stopper.counter,\n",
    "                'metrics': metrics\n",
    "            }, checkpoint_path)\n",
    "\n",
    "            # Save best model\n",
    "            if metrics['val_loss'][-1] == min(metrics['val_loss']):\n",
    "                torch.save(model.state_dict(), f\"best_model_{cb_type}.pth\")\n",
    "                visualize_and_save(orig[0], target[0], output[0], cb_type,\n",
    "                                   epoch, 0, \"best_results\")\n",
    "\n",
    "            # Early stopping check\n",
    "            early_stopper(metrics['val_loss'][-1])\n",
    "            if early_stopper.early_stop:\n",
    "                print(f\"Early stopping {cb_type} at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "            # Print progress\n",
    "            print(f\"[Epoch {epoch+1}/{NUM_EPOCHS}] \"\n",
    "                  f\"Train Loss: {metrics['train_loss'][-1]:.4f}, \"\n",
    "                  f\"Val Loss: {metrics['val_loss'][-1]:.4f}, \"\n",
    "                  f\"Train PSNR: {metrics['train_psnr'][-1]:.2f}, \"\n",
    "                  f\"Val PSNR: {metrics['val_psnr'][-1]:.2f}, \"\n",
    "                  f\"Train LPIPS: {metrics['train_lpips'][-1]:.4f}, \"\n",
    "                  f\"Val LPIPS: {metrics['val_lpips'][-1]:.4f}\")\n",
    "\n",
    "        # Final cleanup after successful completion\n",
    "        np.savez(metric_path, **metrics)\n",
    "        torch.save(model.state_dict(), f\"final_model_{cb_type}.pth\")\n",
    "\n",
    "        # Remove temporary files\n",
    "        if os.path.exists(checkpoint_path):\n",
    "            os.remove(checkpoint_path)\n",
    "\n",
    "        print(f\"\\n{'='*40}\\nTraining completed for {cb_type}\\n{'='*40}\")\n",
    "\n",
    "    finally:\n",
    "        # Cleanup resources without deleting LMDBs\n",
    "        if 'train_dataset' in locals():\n",
    "            train_dataset.close()\n",
    "        if 'val_dataset' in locals():\n",
    "            val_dataset.close()\n",
    "\n",
    "        if 'train_loader' in locals():\n",
    "            del train_loader\n",
    "        if 'val_loader' in locals():\n",
    "            del val_loader\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f24b10ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def force_delete(path):\n",
    "    \"\"\"Robust deletion with Windows lock handling\"\"\"\n",
    "    for _ in range(5):  # 5 retries\n",
    "        try:\n",
    "            if os.path.exists(path):\n",
    "                shutil.rmtree(path, ignore_errors=True)\n",
    "                # Handle Windows lock file explicitly\n",
    "                if os.name == 'nt':\n",
    "                    lock_file = os.path.join(path, 'lock.mdb')\n",
    "                    if os.path.exists(lock_file):\n",
    "                        os.remove(lock_file)\n",
    "                print(f\"Successfully deleted {path}\")\n",
    "                return\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {_+1}/5 failed for {path}: {str(e)}\")\n",
    "            time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0cd4270d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Preprocessing protanopia\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing protanopia: 100%|██████████| 30/30 [03:14<00:00,  6.49s/it]\n",
      "Processing protanopia: 100%|██████████| 3/3 [00:23<00:00,  7.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp_train_protanopia.lmdb has 29670 samples\n",
      "temp_val_protanopia.lmdb has 2967 samples\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Taneem\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Taneem\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: c:\\Users\\Taneem\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "protanopia Epoch 1: 100%|██████████| 1855/1855 [19:21<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/50] Train Loss: 0.1130, Val Loss: 0.0491, Train PSNR: 19.19, Val PSNR: 22.14, Train LPIPS: 0.2612, Val LPIPS: 0.1582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "protanopia Epoch 2: 100%|██████████| 1855/1855 [18:31<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2/50] Train Loss: 0.0610, Val Loss: 0.0295, Train PSNR: 22.44, Val PSNR: 23.87, Train LPIPS: 0.1380, Val LPIPS: 0.1141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "protanopia Epoch 3: 100%|██████████| 1855/1855 [18:32<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3/50] Train Loss: 0.0519, Val Loss: 0.0284, Train PSNR: 23.48, Val PSNR: 24.72, Train LPIPS: 0.1150, Val LPIPS: 0.1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "protanopia Epoch 4: 100%|██████████| 1855/1855 [18:31<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4/50] Train Loss: 0.0488, Val Loss: 0.0239, Train PSNR: 24.00, Val PSNR: 25.27, Train LPIPS: 0.1062, Val LPIPS: 0.0931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "protanopia Epoch 5: 100%|██████████| 1855/1855 [18:25<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5/50] Train Loss: 0.0429, Val Loss: 0.0260, Train PSNR: 24.51, Val PSNR: 25.57, Train LPIPS: 0.0974, Val LPIPS: 0.0926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "protanopia Epoch 6: 100%|██████████| 1855/1855 [18:23<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6/50] Train Loss: 0.0672, Val Loss: 0.0244, Train PSNR: 23.46, Val PSNR: 25.37, Train LPIPS: 0.1169, Val LPIPS: 0.0888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "protanopia Epoch 7: 100%|██████████| 1855/1855 [18:26<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7/50] Train Loss: 0.0547, Val Loss: 0.0224, Train PSNR: 24.25, Val PSNR: 25.65, Train LPIPS: 0.1007, Val LPIPS: 0.0872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "protanopia Epoch 8: 100%|██████████| 1855/1855 [18:31<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8/50] Train Loss: 0.0505, Val Loss: 0.0235, Train PSNR: 24.60, Val PSNR: 25.99, Train LPIPS: 0.0950, Val LPIPS: 0.0866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "protanopia Epoch 9: 100%|██████████| 1855/1855 [18:27<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9/50] Train Loss: 0.0430, Val Loss: 0.0241, Train PSNR: 25.04, Val PSNR: 25.75, Train LPIPS: 0.0885, Val LPIPS: 0.0812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "protanopia Epoch 10: 100%|██████████| 1855/1855 [18:14<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10/50] Train Loss: 0.0360, Val Loss: 0.0183, Train PSNR: 25.41, Val PSNR: 26.62, Train LPIPS: 0.0845, Val LPIPS: 0.0759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "protanopia Epoch 11: 100%|██████████| 1855/1855 [18:21<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 11/50] Train Loss: 0.0290, Val Loss: 0.0173, Train PSNR: 25.83, Val PSNR: 26.81, Train LPIPS: 0.0800, Val LPIPS: 0.0739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "protanopia Epoch 12: 100%|██████████| 1855/1855 [18:17<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 12/50] Train Loss: 0.0308, Val Loss: 0.0181, Train PSNR: 25.97, Val PSNR: 27.08, Train LPIPS: 0.0790, Val LPIPS: 0.0715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "protanopia Epoch 13: 100%|██████████| 1855/1855 [18:22<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 13/50] Train Loss: 0.0276, Val Loss: 0.0158, Train PSNR: 26.23, Val PSNR: 27.27, Train LPIPS: 0.0754, Val LPIPS: 0.0678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "protanopia Epoch 14: 100%|██████████| 1855/1855 [18:25<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 14/50] Train Loss: 0.0267, Val Loss: 0.0156, Train PSNR: 26.37, Val PSNR: 27.18, Train LPIPS: 0.0742, Val LPIPS: 0.0650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "protanopia Epoch 15: 100%|██████████| 1855/1855 [18:20<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 15/50] Train Loss: 0.0230, Val Loss: 0.0145, Train PSNR: 26.73, Val PSNR: 27.53, Train LPIPS: 0.0704, Val LPIPS: 0.0656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "protanopia Epoch 16: 100%|██████████| 1855/1855 [18:25<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 16/50] Train Loss: 0.0220, Val Loss: 0.0137, Train PSNR: 26.90, Val PSNR: 27.62, Train LPIPS: 0.0682, Val LPIPS: 0.0614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "protanopia Epoch 17: 100%|██████████| 1855/1855 [18:29<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 17/50] Train Loss: 0.0226, Val Loss: 0.0134, Train PSNR: 26.98, Val PSNR: 27.81, Train LPIPS: 0.0670, Val LPIPS: 0.0598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "protanopia Epoch 18: 100%|██████████| 1855/1855 [18:19<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 18/50] Train Loss: 0.0201, Val Loss: 0.0129, Train PSNR: 27.23, Val PSNR: 27.92, Train LPIPS: 0.0644, Val LPIPS: 0.0579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "protanopia Epoch 19: 100%|██████████| 1855/1855 [18:23<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 19/50] Train Loss: 0.0195, Val Loss: 0.0130, Train PSNR: 27.35, Val PSNR: 28.02, Train LPIPS: 0.0626, Val LPIPS: 0.0565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "protanopia Epoch 20: 100%|██████████| 1855/1855 [18:21<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 20/50] Train Loss: 0.0193, Val Loss: 0.0145, Train PSNR: 27.46, Val PSNR: 28.03, Train LPIPS: 0.0611, Val LPIPS: 0.0552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "protanopia Epoch 21: 100%|██████████| 1855/1855 [18:22<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 21/50] Train Loss: 0.0182, Val Loss: 0.0123, Train PSNR: 27.61, Val PSNR: 28.35, Train LPIPS: 0.0593, Val LPIPS: 0.0538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "protanopia Epoch 22: 100%|██████████| 1855/1855 [18:24<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 22/50] Train Loss: 0.0181, Val Loss: 0.0124, Train PSNR: 27.70, Val PSNR: 28.17, Train LPIPS: 0.0580, Val LPIPS: 0.0524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "protanopia Epoch 23: 100%|██████████| 1855/1855 [18:20<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 23/50] Train Loss: 0.0177, Val Loss: 0.0123, Train PSNR: 27.81, Val PSNR: 28.63, Train LPIPS: 0.0566, Val LPIPS: 0.0511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "protanopia Epoch 24: 100%|██████████| 1855/1855 [18:36<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 24/50] Train Loss: 0.0170, Val Loss: 0.0125, Train PSNR: 27.92, Val PSNR: 28.24, Train LPIPS: 0.0552, Val LPIPS: 0.0500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "protanopia Epoch 25: 100%|██████████| 1855/1855 [18:25<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 25/50] Train Loss: 0.0170, Val Loss: 0.0117, Train PSNR: 28.00, Val PSNR: 28.43, Train LPIPS: 0.0541, Val LPIPS: 0.0484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "protanopia Epoch 26: 100%|██████████| 1855/1855 [18:22<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping protanopia at epoch 26\n",
      "\n",
      "========================================\n",
      "Training completed for protanopia\n",
      "========================================\n",
      "\n",
      "Cleaning up temp_train_protanopia.lmdb before next type...\n",
      "Successfully deleted temp_train_protanopia.lmdb\n",
      "\n",
      "Cleaning up temp_val_protanopia.lmdb before next type...\n",
      "Successfully deleted temp_val_protanopia.lmdb\n",
      "\n",
      "========================================\n",
      "Preprocessing deuteranopia\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing deuteranopia: 100%|██████████| 30/30 [08:04<00:00, 16.16s/it]\n",
      "Processing deuteranopia: 100%|██████████| 3/3 [00:21<00:00,  7.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp_train_deuteranopia.lmdb has 29670 samples\n",
      "temp_val_deuteranopia.lmdb has 2967 samples\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\Taneem\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "deuteranopia Epoch 1: 100%|██████████| 1855/1855 [19:01<00:00,  1.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/50] Train Loss: 0.1124, Val Loss: 0.0404, Train PSNR: 18.78, Val PSNR: 22.52, Train LPIPS: 0.2796, Val LPIPS: 0.1437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "deuteranopia Epoch 2: 100%|██████████| 1855/1855 [18:34<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2/50] Train Loss: 0.0662, Val Loss: 0.0300, Train PSNR: 22.49, Val PSNR: 24.06, Train LPIPS: 0.1356, Val LPIPS: 0.1100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "deuteranopia Epoch 3: 100%|██████████| 1855/1855 [18:23<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3/50] Train Loss: 0.0539, Val Loss: 0.0252, Train PSNR: 23.55, Val PSNR: 24.97, Train LPIPS: 0.1123, Val LPIPS: 0.0983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "deuteranopia Epoch 4: 100%|██████████| 1855/1855 [18:18<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4/50] Train Loss: 0.0508, Val Loss: 0.0224, Train PSNR: 24.12, Val PSNR: 25.66, Train LPIPS: 0.1051, Val LPIPS: 0.0935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "deuteranopia Epoch 5: 100%|██████████| 1855/1855 [18:27<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5/50] Train Loss: 0.0466, Val Loss: 0.0210, Train PSNR: 24.64, Val PSNR: 25.94, Train LPIPS: 0.0988, Val LPIPS: 0.0858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "deuteranopia Epoch 6: 100%|██████████| 1855/1855 [18:23<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6/50] Train Loss: 0.0454, Val Loss: 0.0265, Train PSNR: 24.88, Val PSNR: 26.09, Train LPIPS: 0.0958, Val LPIPS: 0.0848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "deuteranopia Epoch 7: 100%|██████████| 1855/1855 [18:32<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7/50] Train Loss: 0.0508, Val Loss: 0.0275, Train PSNR: 24.73, Val PSNR: 26.06, Train LPIPS: 0.0989, Val LPIPS: 0.0860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "deuteranopia Epoch 8: 100%|██████████| 1855/1855 [18:37<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8/50] Train Loss: 0.0401, Val Loss: 0.0227, Train PSNR: 25.29, Val PSNR: 26.59, Train LPIPS: 0.0878, Val LPIPS: 0.0793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "deuteranopia Epoch 9: 100%|██████████| 1855/1855 [18:37<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9/50] Train Loss: 0.0502, Val Loss: 0.0215, Train PSNR: 24.98, Val PSNR: 26.62, Train LPIPS: 0.0907, Val LPIPS: 0.0798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "deuteranopia Epoch 10: 100%|██████████| 1855/1855 [18:30<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10/50] Train Loss: 0.0415, Val Loss: 0.0185, Train PSNR: 25.50, Val PSNR: 26.97, Train LPIPS: 0.0836, Val LPIPS: 0.0735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "deuteranopia Epoch 11: 100%|██████████| 1855/1855 [18:30<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 11/50] Train Loss: 0.0326, Val Loss: 0.0173, Train PSNR: 25.92, Val PSNR: 26.98, Train LPIPS: 0.0786, Val LPIPS: 0.0696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "deuteranopia Epoch 12: 100%|██████████| 1855/1855 [18:35<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 12/50] Train Loss: 0.0325, Val Loss: 0.0164, Train PSNR: 26.11, Val PSNR: 27.38, Train LPIPS: 0.0777, Val LPIPS: 0.0674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "deuteranopia Epoch 13: 100%|██████████| 1855/1855 [18:32<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 13/50] Train Loss: 0.0274, Val Loss: 0.0152, Train PSNR: 26.50, Val PSNR: 27.59, Train LPIPS: 0.0723, Val LPIPS: 0.0640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "deuteranopia Epoch 14: 100%|██████████| 1855/1855 [18:27<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 14/50] Train Loss: 0.0308, Val Loss: 0.0238, Train PSNR: 26.45, Val PSNR: 26.97, Train LPIPS: 0.0719, Val LPIPS: 0.0740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "deuteranopia Epoch 15: 100%|██████████| 1855/1855 [18:28<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 15/50] Train Loss: 0.0261, Val Loss: 0.0148, Train PSNR: 26.80, Val PSNR: 27.85, Train LPIPS: 0.0672, Val LPIPS: 0.0585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "deuteranopia Epoch 16: 100%|██████████| 1855/1855 [18:34<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 16/50] Train Loss: 0.0267, Val Loss: 0.0194, Train PSNR: 26.91, Val PSNR: 27.52, Train LPIPS: 0.0663, Val LPIPS: 0.0599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "deuteranopia Epoch 17: 100%|██████████| 1855/1855 [18:29<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 17/50] Train Loss: 0.0231, Val Loss: 0.0157, Train PSNR: 27.20, Val PSNR: 28.08, Train LPIPS: 0.0628, Val LPIPS: 0.0561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "deuteranopia Epoch 18: 100%|██████████| 1855/1855 [18:28<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 18/50] Train Loss: 0.0224, Val Loss: 0.0139, Train PSNR: 27.34, Val PSNR: 28.34, Train LPIPS: 0.0611, Val LPIPS: 0.0534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "deuteranopia Epoch 19: 100%|██████████| 1855/1855 [18:32<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 19/50] Train Loss: 0.0212, Val Loss: 0.0135, Train PSNR: 27.52, Val PSNR: 28.33, Train LPIPS: 0.0589, Val LPIPS: 0.0523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "deuteranopia Epoch 20: 100%|██████████| 1855/1855 [18:40<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 20/50] Train Loss: 0.0208, Val Loss: 0.0130, Train PSNR: 27.60, Val PSNR: 28.34, Train LPIPS: 0.0576, Val LPIPS: 0.0494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "deuteranopia Epoch 21: 100%|██████████| 1855/1855 [18:41<00:00,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 21/50] Train Loss: 0.0191, Val Loss: 0.0134, Train PSNR: 27.82, Val PSNR: 28.42, Train LPIPS: 0.0552, Val LPIPS: 0.0487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "deuteranopia Epoch 22: 100%|██████████| 1855/1855 [18:32<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 22/50] Train Loss: 0.0182, Val Loss: 0.0127, Train PSNR: 27.98, Val PSNR: 28.80, Train LPIPS: 0.0537, Val LPIPS: 0.0468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "deuteranopia Epoch 23: 100%|██████████| 1855/1855 [18:32<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 23/50] Train Loss: 0.0179, Val Loss: 0.0130, Train PSNR: 28.07, Val PSNR: 28.90, Train LPIPS: 0.0525, Val LPIPS: 0.0473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "deuteranopia Epoch 24: 100%|██████████| 1855/1855 [18:39<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 24/50] Train Loss: 0.0170, Val Loss: 0.0129, Train PSNR: 28.22, Val PSNR: 28.84, Train LPIPS: 0.0509, Val LPIPS: 0.0447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "deuteranopia Epoch 25: 100%|██████████| 1855/1855 [18:32<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 25/50] Train Loss: 0.0166, Val Loss: 0.0119, Train PSNR: 28.33, Val PSNR: 29.08, Train LPIPS: 0.0496, Val LPIPS: 0.0437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "deuteranopia Epoch 26: 100%|██████████| 1855/1855 [18:40<00:00,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 26/50] Train Loss: 0.0160, Val Loss: 0.0119, Train PSNR: 28.44, Val PSNR: 29.23, Train LPIPS: 0.0483, Val LPIPS: 0.0427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "deuteranopia Epoch 27: 100%|██████████| 1855/1855 [18:40<00:00,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 27/50] Train Loss: 0.0156, Val Loss: 0.0117, Train PSNR: 28.55, Val PSNR: 29.11, Train LPIPS: 0.0473, Val LPIPS: 0.0420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "deuteranopia Epoch 28: 100%|██████████| 1855/1855 [18:40<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 28/50] Train Loss: 0.0153, Val Loss: 0.0115, Train PSNR: 28.64, Val PSNR: 29.30, Train LPIPS: 0.0464, Val LPIPS: 0.0401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "deuteranopia Epoch 29: 100%|██████████| 1855/1855 [18:40<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 29/50] Train Loss: 0.0148, Val Loss: 0.0119, Train PSNR: 28.74, Val PSNR: 29.38, Train LPIPS: 0.0453, Val LPIPS: 0.0411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "deuteranopia Epoch 30: 100%|██████████| 1855/1855 [18:30<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 30/50] Train Loss: 0.0147, Val Loss: 0.0119, Train PSNR: 28.82, Val PSNR: 29.55, Train LPIPS: 0.0446, Val LPIPS: 0.0418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "deuteranopia Epoch 31: 100%|██████████| 1855/1855 [18:33<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 31/50] Train Loss: 0.0144, Val Loss: 0.0113, Train PSNR: 28.91, Val PSNR: 29.49, Train LPIPS: 0.0438, Val LPIPS: 0.0388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "deuteranopia Epoch 32: 100%|██████████| 1855/1855 [18:43<00:00,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping deuteranopia at epoch 32\n",
      "\n",
      "========================================\n",
      "Training completed for deuteranopia\n",
      "========================================\n",
      "\n",
      "Cleaning up temp_train_deuteranopia.lmdb before next type...\n",
      "Successfully deleted temp_train_deuteranopia.lmdb\n",
      "\n",
      "Cleaning up temp_val_deuteranopia.lmdb before next type...\n",
      "Successfully deleted temp_val_deuteranopia.lmdb\n",
      "\n",
      "========================================\n",
      "Preprocessing tritanopia\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tritanopia: 100%|██████████| 30/30 [10:21<00:00, 20.73s/it]\n",
      "Processing tritanopia: 100%|██████████| 3/3 [00:25<00:00,  8.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp_train_tritanopia.lmdb has 29670 samples\n",
      "temp_val_tritanopia.lmdb has 2967 samples\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\Taneem\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tritanopia Epoch 1: 100%|██████████| 1855/1855 [19:19<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/50] Train Loss: 0.1196, Val Loss: 0.0442, Train PSNR: 19.25, Val PSNR: 22.71, Train LPIPS: 0.2238, Val LPIPS: 0.1178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tritanopia Epoch 2: 100%|██████████| 1855/1855 [18:30<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2/50] Train Loss: 0.0680, Val Loss: 0.0377, Train PSNR: 22.42, Val PSNR: 24.00, Train LPIPS: 0.1178, Val LPIPS: 0.0992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tritanopia Epoch 3: 100%|██████████| 1855/1855 [18:41<00:00,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3/50] Train Loss: 0.0642, Val Loss: 0.0498, Train PSNR: 23.14, Val PSNR: 23.82, Train LPIPS: 0.1073, Val LPIPS: 0.1189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tritanopia Epoch 4: 100%|██████████| 1855/1855 [18:39<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4/50] Train Loss: 0.0616, Val Loss: 0.0455, Train PSNR: 23.48, Val PSNR: 24.46, Train LPIPS: 0.1011, Val LPIPS: 0.0955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tritanopia Epoch 5: 100%|██████████| 1855/1855 [18:31<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5/50] Train Loss: 0.0554, Val Loss: 0.0370, Train PSNR: 23.94, Val PSNR: 24.98, Train LPIPS: 0.0916, Val LPIPS: 0.0895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tritanopia Epoch 6: 100%|██████████| 1855/1855 [18:36<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6/50] Train Loss: 0.0566, Val Loss: 0.0260, Train PSNR: 24.02, Val PSNR: 25.78, Train LPIPS: 0.0912, Val LPIPS: 0.0751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tritanopia Epoch 7: 100%|██████████| 1855/1855 [18:45<00:00,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7/50] Train Loss: 0.0516, Val Loss: 0.0254, Train PSNR: 24.35, Val PSNR: 25.77, Train LPIPS: 0.0858, Val LPIPS: 0.0718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tritanopia Epoch 8: 100%|██████████| 1855/1855 [18:29<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8/50] Train Loss: 0.0508, Val Loss: 0.0264, Train PSNR: 24.45, Val PSNR: 25.81, Train LPIPS: 0.0845, Val LPIPS: 0.0746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tritanopia Epoch 9: 100%|██████████| 1855/1855 [18:28<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9/50] Train Loss: 0.0494, Val Loss: 0.0242, Train PSNR: 24.62, Val PSNR: 26.24, Train LPIPS: 0.0834, Val LPIPS: 0.0697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tritanopia Epoch 10: 100%|██████████| 1855/1855 [18:26<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10/50] Train Loss: 0.0411, Val Loss: 0.0216, Train PSNR: 25.05, Val PSNR: 26.22, Train LPIPS: 0.0758, Val LPIPS: 0.0635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tritanopia Epoch 11: 100%|██████████| 1855/1855 [18:23<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 11/50] Train Loss: 0.0380, Val Loss: 0.0230, Train PSNR: 25.31, Val PSNR: 26.29, Train LPIPS: 0.0728, Val LPIPS: 0.0656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tritanopia Epoch 12: 100%|██████████| 1855/1855 [18:32<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 12/50] Train Loss: 0.0377, Val Loss: 0.0226, Train PSNR: 25.46, Val PSNR: 26.76, Train LPIPS: 0.0703, Val LPIPS: 0.0606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tritanopia Epoch 13: 100%|██████████| 1855/1855 [18:24<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 13/50] Train Loss: 0.0364, Val Loss: 0.0208, Train PSNR: 25.57, Val PSNR: 26.89, Train LPIPS: 0.0688, Val LPIPS: 0.0583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tritanopia Epoch 14: 100%|██████████| 1855/1855 [18:36<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 14/50] Train Loss: 0.0308, Val Loss: 0.0207, Train PSNR: 25.95, Val PSNR: 26.62, Train LPIPS: 0.0649, Val LPIPS: 0.0547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tritanopia Epoch 15: 100%|██████████| 1855/1855 [18:25<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 15/50] Train Loss: 0.0301, Val Loss: 0.0189, Train PSNR: 26.07, Val PSNR: 27.02, Train LPIPS: 0.0642, Val LPIPS: 0.0545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tritanopia Epoch 16: 100%|██████████| 1855/1855 [18:27<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 16/50] Train Loss: 0.0273, Val Loss: 0.0198, Train PSNR: 26.35, Val PSNR: 27.05, Train LPIPS: 0.0608, Val LPIPS: 0.0534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tritanopia Epoch 17: 100%|██████████| 1855/1855 [18:38<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 17/50] Train Loss: 0.0263, Val Loss: 0.0186, Train PSNR: 26.49, Val PSNR: 27.48, Train LPIPS: 0.0590, Val LPIPS: 0.0516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tritanopia Epoch 18: 100%|██████████| 1855/1855 [18:19<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 18/50] Train Loss: 0.0254, Val Loss: 0.0181, Train PSNR: 26.66, Val PSNR: 27.43, Train LPIPS: 0.0570, Val LPIPS: 0.0493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tritanopia Epoch 19: 100%|██████████| 1855/1855 [18:22<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 19/50] Train Loss: 0.0244, Val Loss: 0.0174, Train PSNR: 26.80, Val PSNR: 27.63, Train LPIPS: 0.0554, Val LPIPS: 0.0484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tritanopia Epoch 20: 100%|██████████| 1855/1855 [18:28<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 20/50] Train Loss: 0.0253, Val Loss: 0.0173, Train PSNR: 26.78, Val PSNR: 27.71, Train LPIPS: 0.0553, Val LPIPS: 0.0469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tritanopia Epoch 21: 100%|██████████| 1855/1855 [18:18<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 21/50] Train Loss: 0.0233, Val Loss: 0.0169, Train PSNR: 27.02, Val PSNR: 27.73, Train LPIPS: 0.0524, Val LPIPS: 0.0451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tritanopia Epoch 22: 100%|██████████| 1855/1855 [18:32<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 22/50] Train Loss: 0.0238, Val Loss: 0.0174, Train PSNR: 27.01, Val PSNR: 27.76, Train LPIPS: 0.0522, Val LPIPS: 0.0453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tritanopia Epoch 23: 100%|██████████| 1855/1855 [18:32<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 23/50] Train Loss: 0.0222, Val Loss: 0.0163, Train PSNR: 27.22, Val PSNR: 27.88, Train LPIPS: 0.0502, Val LPIPS: 0.0429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tritanopia Epoch 24: 100%|██████████| 1855/1855 [18:20<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 24/50] Train Loss: 0.0218, Val Loss: 0.0168, Train PSNR: 27.30, Val PSNR: 27.99, Train LPIPS: 0.0491, Val LPIPS: 0.0425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tritanopia Epoch 25: 100%|██████████| 1855/1855 [18:33<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 25/50] Train Loss: 0.0216, Val Loss: 0.0172, Train PSNR: 27.37, Val PSNR: 28.19, Train LPIPS: 0.0480, Val LPIPS: 0.0422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tritanopia Epoch 26: 100%|██████████| 1855/1855 [18:29<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 26/50] Train Loss: 0.0208, Val Loss: 0.0176, Train PSNR: 27.48, Val PSNR: 27.37, Train LPIPS: 0.0466, Val LPIPS: 0.0422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tritanopia Epoch 27: 100%|██████████| 1855/1855 [18:25<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 27/50] Train Loss: 0.0204, Val Loss: 0.0157, Train PSNR: 27.57, Val PSNR: 28.01, Train LPIPS: 0.0457, Val LPIPS: 0.0392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tritanopia Epoch 28: 100%|██████████| 1855/1855 [18:33<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping tritanopia at epoch 28\n",
      "\n",
      "========================================\n",
      "Training completed for tritanopia\n",
      "========================================\n",
      "\n",
      "Cleaning up temp_train_tritanopia.lmdb before next type...\n",
      "Successfully deleted temp_train_tritanopia.lmdb\n",
      "\n",
      "Cleaning up temp_val_tritanopia.lmdb before next type...\n",
      "Successfully deleted temp_val_tritanopia.lmdb\n",
      "\n",
      "Training completed for all types!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    os.makedirs(\"training_samples\", exist_ok=True)\n",
    "    os.makedirs(\"best_results\", exist_ok=True)\n",
    "\n",
    "    for cb_type in COLORBLIND_TYPES:\n",
    "        train_colorblind_type(cb_type)\n",
    "        current_files = [\n",
    "            f\"temp_train_{cb_type}.lmdb\",\n",
    "            f\"temp_val_{cb_type}.lmdb\"\n",
    "        ]\n",
    "        for path in current_files:\n",
    "            print(f\"\\nCleaning up {path} before next type...\")\n",
    "            force_delete(path)\n",
    "\n",
    "    print(\"\\nTraining completed for all types!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4959304",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(metrics, title_suffix=\"\", figsize=(18, 6), dpi=300):\n",
    "    plt.figure(figsize=figsize, dpi=dpi)\n",
    "\n",
    "    # Loss Plot\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(metrics['train_loss'], 'b--', label='Train Loss')\n",
    "    plt.plot(metrics['val_loss'], 'r-', label='Val Loss')\n",
    "    plt.title('1 - MS-SSIM Loss', fontsize=12)\n",
    "    plt.xlabel('Epoch', fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "\n",
    "    # PSNR Plot\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(metrics['train_psnr'], 'g--', label='Train PSNR')\n",
    "    plt.plot(metrics['val_psnr'], 'm-', label='Val PSNR')\n",
    "    plt.title('PSNR (dB)', fontsize=12)\n",
    "    plt.xlabel('Epoch', fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "\n",
    "    # LPIPS Plot\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(metrics['train_lpips'], 'c--', label='Train LPIPS')\n",
    "    plt.plot(metrics['val_lpips'], 'y-', label='Val LPIPS')\n",
    "    plt.title('LPIPS (Lower = Better)', fontsize=12)\n",
    "    plt.xlabel('Epoch', fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.suptitle(f'Training Metrics {title_suffix}', y=1.02, fontsize=14)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    save_path = f\"training_metrics_{title_suffix.replace(' ', '_')}.png\"\n",
    "    plt.savefig(save_path, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Saved metrics plot to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa952d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved metrics plot to training_metrics_(Protanopia).png\n",
      "Saved metrics plot to training_metrics_(Deuteranopia).png\n",
      "Saved metrics plot to training_metrics_(Tritanopia).png\n"
     ]
    }
   ],
   "source": [
    "for cb_type in COLORBLIND_TYPES:\n",
    "    metrics = np.load(f\"metrics_{cb_type}.npz\")\n",
    "    plot_metrics(\n",
    "        metrics,\n",
    "        title_suffix=f\"({cb_type.capitalize()})\",\n",
    "        figsize=(20, 6),\n",
    "        dpi=150\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a787e529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\Taneem\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Taneem\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:62: FutureWarning: Importing `StructuralSimilarityIndexMeasure` from `torchmetrics` was deprecated and will be removed in 2.0. Import `StructuralSimilarityIndexMeasure` from `torchmetrics.image` instead.\n",
      "  _future_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Testing for PROTANOPIA\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing protanopia: 100%|██████████| 3/3 [00:28<00:00,  9.49s/it]\n",
      "Testing protanopia: 100%|██████████| 186/186 [01:22<00:00,  2.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PROTANOPIA Test Results:\n",
      "- Avg MS-SSIM Loss: 0.0113\n",
      "- Avg SSIM: 0.9125\n",
      "- Avg PSNR: 28.39 dB\n",
      "- Avg LPIPS: 0.0478\n",
      "- Inference Speed: 52.73 ms/batch (303.4 FPS)\n",
      "\n",
      "Cleaning up temp_test_protanopia.lmdb before next type...\n",
      "Successfully deleted temp_test_protanopia.lmdb\n",
      "\n",
      "========================================\n",
      "Testing for DEUTERANOPIA\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing deuteranopia: 100%|██████████| 3/3 [00:21<00:00,  7.17s/it]\n",
      "Testing deuteranopia: 100%|██████████| 186/186 [01:21<00:00,  2.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DEUTERANOPIA Test Results:\n",
      "- Avg MS-SSIM Loss: 0.0109\n",
      "- Avg SSIM: 0.9174\n",
      "- Avg PSNR: 29.51 dB\n",
      "- Avg LPIPS: 0.0382\n",
      "- Inference Speed: 52.59 ms/batch (304.2 FPS)\n",
      "\n",
      "Cleaning up temp_test_deuteranopia.lmdb before next type...\n",
      "Successfully deleted temp_test_deuteranopia.lmdb\n",
      "\n",
      "========================================\n",
      "Testing for TRITANOPIA\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tritanopia: 100%|██████████| 3/3 [00:20<00:00,  6.76s/it]\n",
      "Testing tritanopia: 100%|██████████| 186/186 [01:21<00:00,  2.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRITANOPIA Test Results:\n",
      "- Avg MS-SSIM Loss: 0.0157\n",
      "- Avg SSIM: 0.9041\n",
      "- Avg PSNR: 27.98 dB\n",
      "- Avg LPIPS: 0.0392\n",
      "- Inference Speed: 55.71 ms/batch (287.2 FPS)\n",
      "\n",
      "Cleaning up temp_test_tritanopia.lmdb before next type...\n",
      "Successfully deleted temp_test_tritanopia.lmdb\n",
      "\n",
      "All tests completed. Results saved in test_results/ directory.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 16\n",
    "colorblind_types = [\"protanopia\", \"deuteranopia\", \"tritanopia\"]\n",
    "\n",
    "# Initialize metrics with correct data ranges\n",
    "ms_ssim = MS_SSIM(data_range=2.0).to(device)  # For [-1, 1] range\n",
    "psnr = PeakSignalNoiseRatio().to(device)\n",
    "lpips_model = lpips.LPIPS(net='alex').to(device).eval()  # LPIPS in eval mode\n",
    "ssim = torchmetrics.StructuralSimilarityIndexMeasure(\n",
    "    data_range=2.0).to(device)  # Corrected data range\n",
    "\n",
    "\n",
    "for cb_type in colorblind_types:\n",
    "    print(f\"\\n{'='*40}\\nTesting for {cb_type.upper()}\\n{'='*40}\")\n",
    "\n",
    "    test_lmdb_path = f\"temp_test_{cb_type}.lmdb\"\n",
    "    if not os.path.exists(test_lmdb_path):\n",
    "        preprocess_type(\"data/test\", cb_type, test_lmdb_path)\n",
    "\n",
    "    test_dataset = ColorblindDataset(test_lmdb_path, cb_type)\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, batch_size=batch_size, num_workers=0,\n",
    "        shuffle=False, pin_memory=True)\n",
    "\n",
    "    model = Autoencoder().to(device)\n",
    "    model_path = f\"best_model_{cb_type}.pth\"\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    # Initialize metrics\n",
    "    metrics = {\n",
    "        'loss': 0.0,\n",
    "        'psnr': 0.0,\n",
    "        'lpips': 0.0,\n",
    "        'ssim': 0.0,\n",
    "        'times': []\n",
    "    }\n",
    "\n",
    "    save_dir = f\"test_results/{cb_type}\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    img_counter = 0  # For sequential image naming\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (orig, target) in enumerate(tqdm(test_loader, desc=f\"Testing {cb_type}\")):\n",
    "            orig, target = orig.to(device, non_blocking=True), target.to(\n",
    "                device, non_blocking=True)\n",
    "\n",
    "            # Synchronize CUDA for accurate timing\n",
    "            if device.type == 'cuda':\n",
    "                torch.cuda.synchronize()\n",
    "            start = time.time()\n",
    "\n",
    "            output = model(orig)\n",
    "\n",
    "            if device.type == 'cuda':\n",
    "                torch.cuda.synchronize()\n",
    "            metrics['times'].append(time.time() - start)\n",
    "\n",
    "            # Calculate metrics\n",
    "            metrics['loss'] += (1 - ms_ssim(output, target)).item()\n",
    "            metrics['ssim'] += ssim(output, target).item()\n",
    "\n",
    "            # For PSNR: scale to [0, 1]\n",
    "            output_scaled = (output + 1) / 2\n",
    "            target_scaled = (target + 1) / 2\n",
    "            metrics['psnr'] += psnr(output_scaled, target_scaled).item()\n",
    "\n",
    "            # LPIPS calculation (keep in [-1, 1] range)\n",
    "            metrics['lpips'] += lpips_model(output, target).mean().item()\n",
    "\n",
    "            # Save first 100 images for comparison\n",
    "            if img_counter < 100:\n",
    "                for i in range(output.size(0)):\n",
    "                    visualize_and_save(\n",
    "                        orig[i],\n",
    "                        target[i],\n",
    "                        output[i],\n",
    "                        cb_type,\n",
    "                        epoch=0,  # Use 0 for testing phase\n",
    "                        batch_idx=img_counter,\n",
    "                        save_dir=save_dir\n",
    "                    )\n",
    "\n",
    "                    # Save individual images\n",
    "                    orig_img = process_image(orig[i])\n",
    "                    gen_img = process_image(output[i])\n",
    "\n",
    "                    # Convert to BGR for OpenCV and save\n",
    "                    cv2.imwrite(\n",
    "                        os.path.join(\n",
    "                            save_dir, f\"original_{img_counter:04d}.png\"),\n",
    "                        cv2.cvtColor(orig_img, cv2.COLOR_RGB2BGR)\n",
    "                    )\n",
    "                    cv2.imwrite(\n",
    "                        os.path.join(\n",
    "                            save_dir, f\"generated_{img_counter:04d}.png\"),\n",
    "                        cv2.cvtColor(gen_img, cv2.COLOR_RGB2BGR)\n",
    "                    )\n",
    "\n",
    "                    img_counter += 1\n",
    "\n",
    "    # Calculate final metrics\n",
    "    n_batches = len(test_loader)\n",
    "    print(f\"\\n{cb_type.upper()} Test Results:\")\n",
    "    print(f\"- Avg MS-SSIM Loss: {metrics['loss'] / n_batches:.4f}\")\n",
    "    print(f\"- Avg SSIM: {metrics['ssim'] / n_batches:.4f}\")\n",
    "    print(f\"- Avg PSNR: {metrics['psnr'] / n_batches:.2f} dB\")\n",
    "    print(f\"- Avg LPIPS: {metrics['lpips'] / n_batches:.4f}\")\n",
    "\n",
    "    # Timing statistics\n",
    "    avg_time = np.mean(metrics['times']) * 1000\n",
    "    fps = batch_size / (avg_time / 1000)\n",
    "    print(f\"- Inference Speed: {avg_time:.2f} ms/batch ({fps:.1f} FPS)\")\n",
    "\n",
    "    if 'test_dataset' in locals():\n",
    "        test_dataset.close()\n",
    "\n",
    "    current_file = f\"temp_test_{cb_type}.lmdb\"\n",
    "    print(f\"\\nCleaning up {current_file} before next type...\")\n",
    "    force_delete(current_file)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\nAll tests completed. Results saved in test_results/ directory.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
