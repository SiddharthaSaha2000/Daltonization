Enhanching Daltonized Images for Color Blind Deficiency (CVD) with Autoencoder

In a stunning world full of colors, color vision deficiency is the most common difficulty every human being faces. This problem still has no treatment. The subject of this research is to expose artificial intelligence methods as profound studies that aim at material, tangible visualization through images of color blindness in humanity, hence becoming a hope that technology will eventually fill the gap in color perception. The two AI models are the basis for creating and utilizing the project.

The first model is Daltonization, which simulates the perception of color by different kinds of CVD and hence can create image transformations that specifically cater to one's eye problems. Secondly, A CNN-based Autoencoder model trains on various images taken under normal vision color conditions and color vision deficiency (CVD) conditions. Therefore, the aim is to create an autoencoder that will convert any image into a better-colored one that is clearly visible to individuals suffering from CVDs.

The triumph of this task is determined by how well it performs in terms of two criteria. Quantitative measurements allow for an analysis of the degree to which the latest picture reproduces original images through methods like SSIM (Structural similarity index). User testing and feedback obtained by people suffering from CVD constitute quality appraisals for verifying the acceptability of changes made to their visual perception.

We ensure that data collection and utilization are done per strict ethical principles to avoid violating privacy rights or obtaining participants' permission. The primary objective of using converted images is to adhere to and also maintain established accessibility standards for people with Color Vision Deficiency (CVD), therefore rendering them more manageable.
