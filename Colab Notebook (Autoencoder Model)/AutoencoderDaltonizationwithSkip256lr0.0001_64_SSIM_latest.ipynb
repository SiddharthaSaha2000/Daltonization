{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 76369,
     "status": "ok",
     "timestamp": 1715566165451,
     "user": {
      "displayName": "Researchers",
      "userId": "11407451739138435489"
     },
     "user_tz": -360
    },
    "id": "vj4aTkkzT0ME",
    "outputId": "ee734a8c-e326-4d95-945f-819ac28af694"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu May 23 01:50:33 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 546.29                 Driver Version: 546.29       CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                     TCC/WDDM  | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1650 Ti   WDDM  | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   46C    P8               4W /  50W |    183MiB /  4096MiB |     16%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      4468    C+G   ...Programs\\Microsoft VS Code\\Code.exe    N/A      |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "#!unzip /content/drive/MyDrive/data.zip -d /content/drive/MyDrive/\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7031,
     "status": "ok",
     "timestamp": 1715566172480,
     "user": {
      "displayName": "Researchers",
      "userId": "11407451739138435489"
     },
     "user_tz": -360
    },
    "id": "2PKAvokzqiy4",
    "outputId": "708f9e55-4744-4168-bea4-b1808430bfcb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch-msssim in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.0.0)\n",
      "Requirement already satisfied: torch in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pytorch-msssim) (2.3.0+cu121)\n",
      "Requirement already satisfied: filelock in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->pytorch-msssim) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->pytorch-msssim) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->pytorch-msssim) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->pytorch-msssim) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->pytorch-msssim) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->pytorch-msssim) (2024.3.1)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->pytorch-msssim) (2021.4.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch->pytorch-msssim) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch->pytorch-msssim) (2021.12.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch->pytorch-msssim) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch->pytorch-msssim) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pytorch-msssim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchmetrics in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.4.0)\n",
      "Requirement already satisfied: numpy>1.20.0 in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchmetrics) (1.26.4)\n",
      "Requirement already satisfied: packaging>17.1 in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchmetrics) (24.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchmetrics) (2.3.0+cu121)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchmetrics) (0.11.2)\n",
      "Requirement already satisfied: pretty-errors==1.2.25 in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchmetrics) (1.2.25)\n",
      "Requirement already satisfied: colorama in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pretty-errors==1.2.25->torchmetrics) (0.4.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from lightning-utilities>=0.8.0->torchmetrics) (69.2.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.11.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.10.0->torchmetrics) (3.14.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.10.0->torchmetrics) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.10.0->torchmetrics) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.10.0->torchmetrics) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.10.0->torchmetrics) (2024.3.1)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.10.0->torchmetrics) (2021.4.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.10.0->torchmetrics) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.10.0->torchmetrics) (2021.12.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 3858,
     "status": "ok",
     "timestamp": 1715567831050,
     "user": {
      "displayName": "Researchers",
      "userId": "11407451739138435489"
     },
     "user_tz": -360
    },
    "id": "lCWN6KzST908"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Taneem\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torchmetrics\n",
    "from PIL import Image\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from pytorch_msssim import MS_SSIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 516,
     "status": "ok",
     "timestamp": 1715567833641,
     "user": {
      "displayName": "Researchers",
      "userId": "11407451739138435489"
     },
     "user_tz": -360
    },
    "id": "J6CO4jFJUVwM"
   },
   "outputs": [],
   "source": [
    "class ColorblindDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Dataset class for loading pairs of images (original, colorblind-simulated).\"\"\"\n",
    "\n",
    "    def __init__(self, data_dir, colorblind_types, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_dir (str): Path to the directory containing image pairs.\n",
    "            colorblind_types (list): List of colorblindness types to include (e.g., [\"protanopia\", \"deuteranopia\", \"tritanopia\"]).\n",
    "            transform (callable, optional): Optional transform to be applied on images.\n",
    "        \"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        self.colorblind_types = colorblind_types\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "        ])\n",
    "        self.image_paths = self.load_data()\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"Loads image pairs (original, colorblind-simulated).\"\"\"\n",
    "        image_paths = []\n",
    "        for colorblind_type in self.colorblind_types:\n",
    "            type_dir = os.path.join(self.data_dir, colorblind_type)\n",
    "            original_dir = os.path.join(self.data_dir, \"original\")\n",
    "            for filename in os.listdir(type_dir):\n",
    "                if filename.endswith((\".jpg\", \".png\", \".jpeg\")):\n",
    "                    original_path = os.path.join(original_dir, filename)\n",
    "                    colorblind_path = os.path.join(type_dir, filename)\n",
    "                    image_paths.append((original_path, colorblind_path))\n",
    "        return image_paths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Loads and returns an original image and its colorblind-simulated version.\"\"\"\n",
    "        original_path, colorblind_path = self.image_paths[index]\n",
    "\n",
    "        # print(f\"Image loaded: {original_path}\")\n",
    "        # print(f\"Colorblind_image Type: {colorblind_path}\")\n",
    "\n",
    "        original_image = Image.open(original_path).convert(\"RGB\")\n",
    "        colorblind_image = Image.open(colorblind_path).convert(\"RGB\")\n",
    "\n",
    "        # print(\"Original Image Shape (after loading):\", original_image.size)\n",
    "        # print(\"Colorblind Image Shape (after loading):\", colorblind_image.size)\n",
    "\n",
    "        if self.transform:\n",
    "            original_image = self.transform(original_image)\n",
    "            colorblind_image = self.transform(colorblind_image)\n",
    "            # print(\"Original Image Shape (after preprocessing):\", original_image.shape)\n",
    "            # print(\"Colorblind Image Shape (after preprocessing):\", colorblind_image.shape)\n",
    "\n",
    "        return original_image, colorblind_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1715567837279,
     "user": {
      "displayName": "Researchers",
      "userId": "11407451739138435489"
     },
     "user_tz": -360
    },
    "id": "j3vlno0RUa_I",
    "outputId": "2fd43fce-b2c4-4277-bffb-0d53da64c443"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"mps\" if torch.backends.mps.is_available(\n",
    ") else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1715567838959,
     "user": {
      "displayName": "Researchers",
      "userId": "11407451739138435489"
     },
     "user_tz": -360
    },
    "id": "Ad83iZa8VrqY"
   },
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.enc1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=2,\n",
    "                      padding=1),  # Output: 32x128x128\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.enc2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2,\n",
    "                      padding=1),  # Output: 64x64x64\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.enc3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2,\n",
    "                      padding=1),  # Output: 128x32x32\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.enc4 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=2,\n",
    "                      padding=1),  # Output: 256x16x16\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.enc5 = nn.Sequential(\n",
    "            # Output: 512x8x8 (Latent Representation)\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        # Decoder (with skip connections)\n",
    "        self.dec5 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=4,\n",
    "                               stride=2, padding=1),  # Output: 256x16x16\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.dec4 = nn.Sequential(\n",
    "            # Output: 128x32x32 (Concatenated: 512 -> 256)\n",
    "            nn.ConvTranspose2d(256 * 2, 128, kernel_size=4,\n",
    "                               stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.dec3 = nn.Sequential(\n",
    "            # Output: 64x64x64 (Concatenated: 256 -> 128)\n",
    "            nn.ConvTranspose2d(128 * 2, 64, kernel_size=4,\n",
    "                               stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.dec2 = nn.Sequential(\n",
    "            # Output: 32x128x128 (Concatenated: 128 -> 64)\n",
    "            nn.ConvTranspose2d(64 * 2, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.dec1 = nn.Sequential(\n",
    "            # Output: 3x256x256 (Concatenated: 64 -> 32)\n",
    "            nn.ConvTranspose2d(32 * 2, 3, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc1 = self.enc1(x)\n",
    "        enc2 = self.enc2(enc1)\n",
    "        enc3 = self.enc3(enc2)\n",
    "        enc4 = self.enc4(enc3)\n",
    "        enc5 = self.enc5(enc4)\n",
    "\n",
    "        dec5 = self.dec5(enc5)\n",
    "        dec4 = self.dec4(torch.cat([dec5, enc4], 1))  # Skip connection\n",
    "        dec3 = self.dec3(torch.cat([dec4, enc3], 1))  # Skip connection\n",
    "        dec2 = self.dec2(torch.cat([dec3, enc2], 1))  # Skip connection\n",
    "        dec1 = self.dec1(torch.cat([dec2, enc1], 1))  # Skip connection\n",
    "        return dec1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1715567843223,
     "user": {
      "displayName": "Researchers",
      "userId": "11407451739138435489"
     },
     "user_tz": -360
    },
    "id": "xy1eAtCCUdde"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Taneem\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:62: FutureWarning: Importing `StructuralSimilarityIndexMeasure` from `torchmetrics` was deprecated and will be removed in 2.0. Import `StructuralSimilarityIndexMeasure` from `torchmetrics.image` instead.\n",
      "  _future_warning(\n"
     ]
    }
   ],
   "source": [
    "def preprocess_image(image):\n",
    "    # Check if additional preprocessing is needed\n",
    "    if image.dtype != torch.float32:\n",
    "        image = image.to(torch.float32)\n",
    "\n",
    "    image = (image + 1) * 0.5\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "ssim = torchmetrics.StructuralSimilarityIndexMeasure(data_range=1.0).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "ms_ssim_value = MS_SSIM(data_range=1, size_average=True, channel=3)\n",
    "\n",
    "\n",
    "def calculate_loss(model_output, colorblind_image):\n",
    "    return 1-ms_ssim_value(model_output, colorblind_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 545,
     "status": "ok",
     "timestamp": 1715567845969,
     "user": {
      "displayName": "Researchers",
      "userId": "11407451739138435489"
     },
     "user_tz": -360
    },
    "id": "zdqgZbmMX-Ms"
   },
   "outputs": [],
   "source": [
    "def visualize_images(input_image, real_image, output_image, target_colorblind_type, iteration):\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    # Convert tensors to NumPy (assuming they are on the appropriate device)\n",
    "    input_image_np = input_image[0].cpu().permute(1, 2, 0).detach().numpy()\n",
    "    real_image_np = real_image[0].cpu().permute(1, 2, 0).detach().numpy()\n",
    "    output_image_np = output_image[0].cpu().permute(1, 2, 0).detach().numpy()\n",
    "\n",
    "    # Ensure image data is in the range [0, 1]\n",
    "    input_image_np = np.clip(input_image_np, 0, 1)\n",
    "    real_image_np = np.clip(real_image_np, 0, 1)\n",
    "    output_image_np = np.clip(output_image_np, 0, 1)\n",
    "\n",
    "    # input_image_np = cv2.resize(input_image_np, dsize=(512, 512), interpolation=cv2.INTER_CUBIC)\n",
    "    # real_image_np = cv2.resize(real_image_np, dsize=(512, 512), interpolation=cv2.INTER_CUBIC)\n",
    "    # output_image_np = cv2.resize(output_image_np, dsize=(512, 512), interpolation=cv2.INTER_CUBIC)\n",
    "    # print(\"Original Image Shape:\", input_image_np.shape)\n",
    "    # print(\"Target Image Shape:\", real_image_np.shape)\n",
    "    # print(\"Output Image Shape:\", output_image_np.shape)\n",
    "\n",
    "    ax1.imshow(input_image_np)\n",
    "    ax1.axis('off')\n",
    "    ax1.set_title('Original Image')\n",
    "\n",
    "    ax2.imshow(real_image_np)\n",
    "    ax2.axis('off')\n",
    "    ax2.set_title(f'Target ({target_colorblind_type})')\n",
    "\n",
    "    ax3.imshow(output_image_np)\n",
    "    ax3.axis('off')\n",
    "    ax3.set_title(f'Generated Image ({target_colorblind_type})')\n",
    "    # plt.savefig(f\"generated_image_{target_colorblind_type}_{iteration}.jpg\")  # Save the figure\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "XS04RFcOXv65",
    "outputId": "db2ae079-ecec-4d38-ab30-bbba1ab02cf4"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "num_epochs = 50\n",
    "batch_size = 64\n",
    "learning_rate = 0.0001\n",
    "\n",
    "# Create the Autoencoder model\n",
    "model = Autoencoder().to(device)\n",
    "# print(model)\n",
    "optimizer = optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "colorblind_types = [\"protanopia\", \"tritanopia\", \"deuteranopia\"]\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Training Loop\n",
    "total_iterations = 0\n",
    "train_losses_epoch = []  # Average loss per epoch (training)\n",
    "train_ssim_epoch = []    # Average SSIM values per epoch (training)\n",
    "val_losses_epoch = []    # Average loss per epoch (validation)\n",
    "val_ssim_epoch = []      # Average SSIM values per epoch (validation)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    data_dir = \"data/train\"\n",
    "    dataset = ColorblindDataset(data_dir, colorblind_types, transform)\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_ssim = []\n",
    "\n",
    "    for i, (image, colorblind_image) in enumerate(dataloader):\n",
    "        original_images = preprocess_image(image).to(device)\n",
    "        colorblind_images = preprocess_image(colorblind_image).to(device)\n",
    "        target_colorblind_type = colorblind_types[i % len(colorblind_types)]\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # print(\"Original Image Range:\", original_images.min(), original_images.max())\n",
    "        # print(\"Target Image Range:\", colorblind_images.min(), colorblind_images.max())\n",
    "        # Forward pass (reconstruction)\n",
    "        output_images = model(original_images)\n",
    "\n",
    "        # print(\"Output Image Shape:\", output_images.shape)\n",
    "        # print(\"Output Image Range:\", output_images.min(), output_images.max())\n",
    "\n",
    "        # Compute loss\n",
    "        loss = calculate_loss(output_images, colorblind_images)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_iterations += 1\n",
    "\n",
    "        train_ssim_value = ssim(output_images, colorblind_images)\n",
    "\n",
    "        # Print loss and SSIM for each iteration (not stored)\n",
    "        print(f\"Epoch: {epoch + 1}/{num_epochs}, Type: {target_colorblind_type}, Iteration: {\n",
    "              i+1}, Training Loss: {loss.item():.4f}, SSIM: {train_ssim_value.item():.4f}\")\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_ssim.append(train_ssim_value.item())\n",
    "\n",
    "    # Calculate average loss and SSIM for the epoch (training) and store for plotting\n",
    "    avg_train_loss = train_loss / len(dataloader)\n",
    "    avg_train_ssim = sum(train_ssim) / len(train_ssim)\n",
    "    train_losses_epoch.append(avg_train_loss)\n",
    "    train_ssim_epoch.append(avg_train_ssim)\n",
    "\n",
    "    visualize_images(original_images, colorblind_images,\n",
    "                     output_images, target_colorblind_type, total_iterations-1)\n",
    "\n",
    "    val_data_dir = \"data/val\"\n",
    "    val_dataset = ColorblindDataset(val_data_dir, colorblind_types, transform)\n",
    "    val_dataloader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    val_loss = 0\n",
    "    val_ssim = []\n",
    "    with torch.no_grad():  # No need to calculate gradients for validation\n",
    "        for image, colorblind_image in val_dataloader:\n",
    "            original_images = preprocess_image(image).to(device)\n",
    "            colorblind_images = preprocess_image(colorblind_image).to(device)\n",
    "            output_images = model(original_images)\n",
    "            loss = calculate_loss(output_images, colorblind_images)\n",
    "\n",
    "            val_ssim_value = ssim(output_images, colorblind_images)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            val_ssim.append(val_ssim_value.item())\n",
    "\n",
    "        target_colorblind_type = colorblind_types[i % len(colorblind_types)]\n",
    "        visualize_images(original_images, colorblind_images,\n",
    "                         output_images, target_colorblind_type, 0)\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_dataloader)\n",
    "    avg_val_ssim = sum(val_ssim) / len(val_ssim)\n",
    "    val_ssim_epoch.append(avg_val_ssim)\n",
    "    val_losses_epoch.append(avg_val_loss)\n",
    "    print(f\"\\nEpoch: {epoch + 1}/{num_epochs}, Training Loss: {avg_train_loss:.4f}, Training SSIM: {\n",
    "          avg_train_ssim:.4f},\\nValidation Loss: {avg_val_loss:.4f},Validation SSIM: {avg_val_ssim:.4f}\\n\")\n",
    "\n",
    "print(\"Training Completed Successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wXUrfS7wiD6j"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses_epoch, label='Training Loss', linestyle='--')\n",
    "plt.plot(val_losses_epoch, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss [1-MSSIM] Over Epochs')\n",
    "plt.legend()\n",
    "plt.savefig('MSSIM_Loss_Graph.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_ssim_epoch, label='Training SSIM',\n",
    "         color='blue', linestyle='--')\n",
    "plt.plot(val_ssim_epoch, label='Validation SSIM',\n",
    "         color='green')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('SSIM')\n",
    "plt.title('Training and Validation SSIM Over Epochs')\n",
    "plt.legend()\n",
    "plt.savefig('SSIM_Accuracy_Graph.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to /AutoencoderDaltonized/trainedAutoencoder256_SSIM_Model_latest.pth\n"
     ]
    }
   ],
   "source": [
    "model = Autoencoder().to(device)\n",
    "model_save_path = \"/AutoencoderDaltonized/trainedAutoencoder256_SSIM_Model_latest.pth\"\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "print(f\"Model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Load your trained model\n",
    "model_path = \"/AutoencoderDaltonized/trainedAutoencoder256_SSIM_Model_latest.pth\"\n",
    "model = Autoencoder().to(device)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "\n",
    "# Test Data\n",
    "colorblind_types = [\"protanopia\", \"tritanopia\", \"deuteranopia\"]\n",
    "\n",
    "test_transform = transforms.Compose([transforms.Resize((256, 256)), transforms.ToTensor(), transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "                                     ])\n",
    "\n",
    "test_data_dir = \"data/test\"\n",
    "test_dataset = ColorblindDataset(\n",
    "    test_data_dir, colorblind_types, test_transform)\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "test_losses = []\n",
    "test_ssims = []\n",
    "total_forward_time = 0\n",
    "num_iterations = 0\n",
    "\n",
    "# Testing Loop\n",
    "with torch.no_grad():\n",
    "    for i, (image, colorblind_image) in enumerate(test_dataloader):\n",
    "        original_images = preprocess_image(image).to(device)\n",
    "        colorblind_images = preprocess_image(colorblind_image).to(device)\n",
    "\n",
    "        # Start timing\n",
    "        start_time = time.time()\n",
    "\n",
    "        output_images = model(original_images)\n",
    "\n",
    "        # End timing\n",
    "        end_time = time.time()\n",
    "        forward_time = end_time - start_time\n",
    "\n",
    "        # Accumulate forward pass time and count iterations\n",
    "        total_forward_time += forward_time\n",
    "        num_iterations += 1\n",
    "\n",
    "        # Loss and SSIM Calculation\n",
    "        loss = calculate_loss(output_images, colorblind_images)\n",
    "        ssim_value = ssim(output_images, colorblind_images)\n",
    "\n",
    "        test_losses.append(loss.item())\n",
    "        test_ssims.append(ssim_value.item())\n",
    "\n",
    "        target_colorblind_type = colorblind_types[i % len(colorblind_types)]\n",
    "\n",
    "    visualize_images(original_images, colorblind_images,\n",
    "                     output_images, target_colorblind_type, 0)\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "# Aggregate and Report Results\n",
    "avg_test_loss = sum(test_losses) / len(test_losses)\n",
    "avg_test_ssim = sum(test_ssims) / len(test_ssims)\n",
    "\n",
    "# Calculate average forward pass time\n",
    "avg_forward_time_per_batch = total_forward_time / num_iterations\n",
    "avg_forward_time_per_image = avg_forward_time_per_batch / batch_size\n",
    "\n",
    "print(f\"\\nAverage Test Loss: {avg_test_loss:.4f}\")\n",
    "print(f\"Average Test SSIM: {avg_test_ssim:.4f}\")\n",
    "print(f\"Average Forward Pass Time per Batch: {\n",
    "      avg_forward_time_per_batch:.4f} seconds\")\n",
    "print(f\"Average Forward Pass Time per Image: {\n",
    "      avg_forward_time_per_image:.6f} seconds\\n\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
